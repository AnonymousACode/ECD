Conditioning on ['alpha']
Entropy of n_nodes: H[N] -2.4754221439361572
/cpfs01/projects-HDD/cfff-4405968bce88_HDD/anjunyi/mole_diffusion_sorce/df_mole/qm9/models.py:139: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  probs = Categorical(torch.tensor(probs))
alphas2 [9.99990000e-01 9.99988000e-01 9.99982000e-01 ... 2.59676966e-05
 1.39959211e-05 1.00039959e-05]
gamma [-11.51291546 -11.33059532 -10.92513058 ...  10.55863126  11.17673063
  11.51251595]
Training using 2 GPUs
Traceback (most recent call last):
  File "/cpfs01/projects-HDD/cfff-4405968bce88_HDD/anjunyi/mole_diffusion_sorce/df_mole/main_qm9.py", line 290, in <module>
    main()
  File "/cpfs01/projects-HDD/cfff-4405968bce88_HDD/anjunyi/mole_diffusion_sorce/df_mole/main_qm9.py", line 241, in main
    train_epoch(args=args, loader=dataloaders['train'], epoch=epoch, model=model, model_dp=model_dp,
  File "/cpfs01/projects-HDD/cfff-4405968bce88_HDD/anjunyi/mole_diffusion_sorce/df_mole/train_test.py", line 53, in train_epoch
    nll, reg_term, mean_abs_z = losses.compute_loss_and_nll(args, model_dp, nodes_dist,
  File "/cpfs01/projects-HDD/cfff-4405968bce88_HDD/anjunyi/mole_diffusion_sorce/df_mole/qm9/losses.py", line 23, in compute_loss_and_nll
    nll = generative_model(x, h, node_mask, edge_mask, context)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/data_parallel.py", line 185, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/data_parallel.py", line 200, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/parallel_apply.py", line 108, in parallel_apply
    output.reraise()
  File "/usr/local/lib/python3.10/dist-packages/torch/_utils.py", line 705, in reraise
    raise exception
AssertionError: Caught AssertionError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/cpfs01/projects-HDD/cfff-4405968bce88_HDD/anjunyi/mole_diffusion_sorce/df_mole/equivariant_diffusion/en_diffusion.py", line 710, in forward
    loss, loss_dict = self.compute_loss(x, h, node_mask, edge_mask, context, t0_always=False)
  File "/cpfs01/projects-HDD/cfff-4405968bce88_HDD/anjunyi/mole_diffusion_sorce/df_mole/equivariant_diffusion/en_diffusion.py", line 607, in compute_loss
    diffusion_utils.assert_mean_zero_with_mask(z_t[:, :, :self.n_dims], node_mask)
  File "/cpfs01/projects-HDD/cfff-4405968bce88_HDD/anjunyi/mole_diffusion_sorce/df_mole/equivariant_diffusion/utils.py", line 51, in assert_mean_zero_with_mask
    assert rel_error < 1e-2, f'Mean is not zero, relative_error {rel_error}'
AssertionError: Mean is not zero, relative_error 1.7608303492073525
